{"cells":[{"cell_type":"markdown","metadata":{"id":"te9Go_TkKH2R"},"source":["# **Analysis of Art - Reproduction (VQGAN + CLIP)**"]},{"cell_type":"markdown","metadata":{"id":"1T1OgZJTKO1q"},"source":["## Download"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":171213,"status":"ok","timestamp":1644105339780,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"rQTmWFVLKGt4","outputId":"3e5a7e24-7356-48f2-9d8f-15c29abdf914"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CLIP'...\n","remote: Enumerating objects: 195, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 195 (delta 12), reused 22 (delta 9), pack-reused 168\u001b[K\n","Receiving objects: 100% (195/195), 8.91 MiB | 22.36 MiB/s, done.\n","Resolving deltas: 100% (92/92), done.\n","Cloning into 'taming-transformers'...\n","remote: Enumerating objects: 1335, done.\u001b[K\n","remote: Counting objects: 100% (525/525), done.\u001b[K\n","remote: Compressing objects: 100% (493/493), done.\u001b[K\n","remote: Total 1335 (delta 58), reused 479 (delta 30), pack-reused 810\u001b[K\n","Receiving objects: 100% (1335/1335), 412.35 MiB | 38.55 MiB/s, done.\n","Resolving deltas: 100% (267/267), done.\n","Collecting ray\n","  Downloading ray-1.10.0-cp37-cp37m-manylinux2014_x86_64.whl (59.6 MB)\n","\u001b[K     |████████████████████████████████| 59.6 MB 1.2 MB/s \n","\u001b[?25hCollecting taming-transformers\n","  Downloading taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 3.6 MB/s \n","\u001b[?25hCollecting ftfy\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Collecting omegaconf\n","  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s \n","\u001b[?25hCollecting pytorch-lightning\n","  Downloading pytorch_lightning-1.5.9-py3-none-any.whl (527 kB)\n","\u001b[K     |████████████████████████████████| 527 kB 46.1 MB/s \n","\u001b[?25hCollecting kornia\n","  Downloading kornia-0.6.3-py2.py3-none-any.whl (474 kB)\n","\u001b[K     |████████████████████████████████| 474 kB 45.8 MB/s \n","\u001b[?25hCollecting imageio-ffmpeg\n","  Downloading imageio_ffmpeg-0.4.5-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[K     |████████████████████████████████| 26.9 MB 1.5 MB/s \n","\u001b[?25hCollecting einops\n","  Downloading einops-0.4.0-py3-none-any.whl (28 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n","Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n","Collecting redis>=3.5.0\n","  Downloading redis-4.1.2-py3-none-any.whl (173 kB)\n","\u001b[K     |████████████████████████████████| 173 kB 51.3 MB/s \n","\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n","Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.4.2)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n","Collecting deprecated>=1.2.3\n","  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray) (4.10.1)\n","Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray) (21.3)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray) (1.13.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis>=3.5.0->ray) (3.0.7)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (0.11.1+cu111)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (1.10.0+cu111)\n","Collecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 51.7 MB/s \n","\u001b[?25hCollecting pyyaml\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 45.2 MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 39.3 MB/s \n","\u001b[?25hCollecting setuptools==59.5.0\n","  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n","\u001b[K     |████████████████████████████████| 952 kB 47.9 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.7.1-py3-none-any.whl (397 kB)\n","\u001b[K     |████████████████████████████████| 397 kB 53.3 MB/s \n","\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 51.9 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 44.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 51.6 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 51.1 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.11)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->taming-transformers) (7.1.2)\n","Building wheels for collected packages: antlr4-python3-runtime, future, ftfy\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=e2849b31e36ca5ea278acdf44a192d45154b50e9140a19fff14543e381689baf\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=7f86cba4324cfaa12761994d24e4aede78ca40e8108ae50733b018ccfd6a17a3\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=5d1cf4c925eb88f5aebf01a3824618b7e4b3424425018040f8b00a558244dd04\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","Successfully built antlr4-python3-runtime future ftfy\n","Installing collected packages: setuptools, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, pyyaml, future, deprecated, antlr4-python3-runtime, redis, pytorch-lightning, omegaconf, taming-transformers, ray, kornia, imageio-ffmpeg, ftfy, einops\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 deprecated-1.2.13 einops-0.4.0 frozenlist-1.3.0 fsspec-2022.1.0 ftfy-6.0.3 future-0.18.2 imageio-ffmpeg-0.4.5 kornia-0.6.3 multidict-6.0.2 omegaconf-2.1.1 pyDeprecate-0.3.1 pytorch-lightning-1.5.9 pyyaml-6.0 ray-1.10.0 redis-4.1.2 setuptools-59.5.0 taming-transformers-0.0.1 torchmetrics-0.7.1 yarl-1.7.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources","pydevd_plugins"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--2022-02-05 23:53:40--  http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml\n","Resolving eaidata.bmk.sh (eaidata.bmk.sh)... 176.9.113.70\n","Connecting to eaidata.bmk.sh (eaidata.bmk.sh)|176.9.113.70|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 920 [application/octet-stream]\n","Saving to: ‘wikiart_16384.yaml’\n","\n","wikiart_16384.yaml  100%[===================>]     920  --.-KB/s    in 0s      \n","\n","2022-02-05 23:53:41 (120 MB/s) - ‘wikiart_16384.yaml’ saved [920/920]\n","\n","--2022-02-05 23:53:41--  http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt\n","Resolving eaidata.bmk.sh (eaidata.bmk.sh)... 176.9.113.70\n","Connecting to eaidata.bmk.sh (eaidata.bmk.sh)|176.9.113.70|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1005327541 (959M) [application/octet-stream]\n","Saving to: ‘wikiart_16384.ckpt’\n","\n","wikiart_16384.ckpt  100%[===================>] 958.75M  7.74MB/s    in 1m 58s  \n","\n","2022-02-05 23:55:39 (8.13 MB/s) - ‘wikiart_16384.ckpt’ saved [1005327541/1005327541]\n","\n"]}],"source":["!git clone https://github.com/openai/CLIP\n","!git clone https://github.com/CompVis/taming-transformers.git\n","!pip install ray taming-transformers ftfy regex tqdm omegaconf pytorch-lightning kornia imageio-ffmpeg einops           \n","!wget -nc -O wikiart_16384.yaml http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml\n","!wget -nc -O wikiart_16384.ckpt http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt \n","!mkdir steps"]},{"cell_type":"markdown","metadata":{"id":"G15xeBYaKfI4"},"source":["## Library"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10739,"status":"ok","timestamp":1644105350498,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"NvbNUXlEKhDI"},"outputs":[],"source":["# Python\n","import argparse\n","import functools\n","import glob\n","import math\n","from pathlib import Path\n","import os\n","import os.path as osp\n","import sys\n","\n","# PyTorch\n","import torch\n","from torch import nn, optim\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Various\n","from base64 import b64encode\n","from CLIP import clip\n","import cv2\n","from google.colab import drive\n","import imageio\n","from IPython import display\n","import kornia.augmentation as K\n","import numpy as np\n","from omegaconf import OmegaConf\n","from PIL import ImageFile, Image\n","from ray import tune\n","from ray.tune import schedulers\n","from taming.models import cond_transformer, vqgan\n","import taming.modules \n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"HCNJc-r_Kmxa"},"source":["## Setup"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644105350499,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"_2Zp-he8KsAk"},"outputs":[],"source":["sys.path.insert(1, '/content/taming-transformers')\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":175,"status":"ok","timestamp":1644105350669,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"3XvlJ8V4Ksvu"},"outputs":[],"source":["def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n","\n","def lanczos(x, a):\n","    cond = torch.logical_and(-a < x, x < a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n","\n","\n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n","\n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n","\n","    input = input.view([n * c, 1, h, w])\n","\n","    if dh < h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n","\n","    if dw < w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n","\n","    input = input.view([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n","\n","def vector_quantize(x, codebook):\n","    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n","    indices = d.argmin(-1)\n","    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n","    return replace_grad(x_q, x)\n","\n","def parse_prompt(prompt):\n","    vals = prompt.rsplit(':', 2)\n","    vals = vals + ['', '1', '-inf'][len(vals):]\n","    return vals[0], float(vals[1]), float(vals[2])\n","\n","def load_vqgan_model(config_path, checkpoint_path):\n","    config = OmegaConf.load(config_path)\n","    if config.model.target == 'taming.models.vqgan.VQModel':\n","        model = vqgan.VQModel(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n","        model = vqgan.GumbelVQ(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n","        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n","        parent_model.eval().requires_grad_(False)\n","        parent_model.init_from_ckpt(checkpoint_path)\n","        model = parent_model.first_stage_model\n","    else:\n","        raise ValueError(f'unknown model type: {config.model.target}')\n","    del model.loss\n","    return model\n","\n","def resize_image(image, out_size):\n","    ratio = image.size[0] / image.size[1]\n","    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n","    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n","    return image.resize(size, Image.LANCZOS)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1644105350670,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"2H28rX_NKvBz"},"outputs":[],"source":["class ReplaceGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x_forward, x_backward):\n","        ctx.shape = x_backward.shape\n","        return x_forward\n","\n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        return None, grad_in.sum_to_size(ctx.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1644105350670,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"ZHvAgT1sKxNI"},"outputs":[],"source":["class ClampWithGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, min, max):\n","        ctx.min = min\n","        ctx.max = max\n","        ctx.save_for_backward(input)\n","        return input.clamp(min, max)\n","\n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        input, = ctx.saved_tensors\n","        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1644105350670,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"Io71HLRsKzPE"},"outputs":[],"source":["class Prompt(nn.Module):\n","    def __init__(self, embed, weight=1., stop=float('-inf')):\n","        super().__init__()\n","        self.register_buffer('embed', embed)\n","        self.register_buffer('weight', torch.as_tensor(weight))\n","        self.register_buffer('stop', torch.as_tensor(stop))\n","\n","    def forward(self, input):\n","        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n","        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n","        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n","        dists = dists * self.weight.sign()\n","        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1644105350931,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"Sp9FnEKDK00G"},"outputs":[],"source":["class MakeCutouts(nn.Module):\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","\n","        self.augs = nn.Sequential(\n","            # K.RandomHorizontalFlip(p=0.5),\n","            # K.RandomVerticalFlip(p=0.5),\n","            # K.RandomSolarize(0.01, 0.01, p=0.7),\n","            # K.RandomSharpness(0.3,p=0.4),\n","            # K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5),\n","            # K.RandomCrop(size=(self.cut_size,self.cut_size), p=0.5),\n","            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n","            K.RandomPerspective(0.7,p=0.7),\n","            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n","            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),\n","            \n",")\n","        self.noise_fac = 0.1\n","        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n","        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n","\n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","        \n","        for _ in range(self.cutn):\n","\n","            # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","            # offsetx = torch.randint(0, sideX - size + 1, ())\n","            # offsety = torch.randint(0, sideY - size + 1, ())\n","            # cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","            # cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","\n","            # cutout = transforms.Resize(size=(self.cut_size, self.cut_size))(input)\n","            \n","            cutout = (self.av_pool(input) + self.max_pool(input))/2\n","            cutouts.append(cutout)\n","        batch = self.augs(torch.cat(cutouts, dim=0))\n","        if self.noise_fac:\n","            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n","            batch = batch + facs * torch.randn_like(batch)\n","        return batch"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644105350932,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"OCUCNwALK3jG"},"outputs":[],"source":["replace_grad = ReplaceGrad.apply\n","clamp_with_grad = ClampWithGrad.apply"]},{"cell_type":"markdown","metadata":{"id":"wkJK2MxbK7PS"},"source":["## Interactive"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1644105350933,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"},"user_tz":-60},"id":"VRlF3vgrLPzs","cellView":"form"},"outputs":[],"source":["#@markdown #**Parameters**\n","#@markdown ---\n","\n","texts = \"romanticism caspar david friedrich moon\" #@param {type:\"string\"}\n","width =  300#@param {type:\"number\"}\n","height = 300#@param {type:\"number\"}\n","model = \"wikiart_16384\" #@param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"vqgan_openimages_f16_8192\", \"wikiart_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"]\n","images_interval =  50#@param {type:\"number\"}\n","init_image = \"\"#@param {type:\"string\"}\n","target_images = \"\"#@param {type:\"string\"}\n","seed = -1#@param {type:\"number\"}\n","max_iterations = -1#@param {type:\"number\"}\n","\n","model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", 'vqgan_openimages_f16_8192':'OpenImages 8912',\n","                 \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\"}\n","name_model = model_names[model]     \n","\n","if seed == -1:\n","    seed = None\n","if init_image == \"None\":\n","    init_image = None\n","if target_images == \"None\" or not target_images:\n","    target_images = []\n","else:\n","    target_images = target_images.split(\"|\")\n","    target_images = [image.strip() for image in target_images]\n","\n","texts = [phrase.strip() for phrase in texts.split(\"|\")]\n","if texts == ['']:\n","    texts = []\n","\n","\n","args = argparse.Namespace(\n","    prompts=texts,\n","    image_prompts=target_images,\n","    noise_prompt_seeds=[],\n","    noise_prompt_weights=[],\n","    size=[width, height],\n","    init_image=init_image,\n","    init_weight=0.,\n","    clip_model='ViT-B/32',\n","    vqgan_config=f'{model}.yaml',\n","    vqgan_checkpoint=f'{model}.ckpt',\n","    step_size=0.1,\n","    cutn=32,\n","    cut_pow=1.,\n","    display_freq=images_interval,\n","    seed=seed,\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1UZ09jMeiKybbLbcvoPCPeNrYepfHjtpT","referenced_widgets":["5fb0728f22b44f0e910cd3e414c79c99","c6ec54b2a8c542ebbe0b827484b12846","2e6d4bb1c6344799be3d62e8f9e7725d","63a2430a64a04e1291866f8866d07c15","f47b2945c09d4d6fa31bb869d66cf9b6","1c48006e2dee4033b808cb2216d7ca14","5e75663fa07144d6b8b0b784d4c42d6e","fc891e7ddb9943a88a9803673999917f","ce5bd45b0b6e4ed2b955e7eb879198c4","ed1e9e38d3de4f7e8741ed6384f1b9a3","4e2eff22a0264fdf8189ccb4d3d27a0c","7b46fc455e5a4750aff9334ae3029f38","14c541b96cbd4026afd45258e3634f07","9e669996c88d4668a7298dd89d61e1a2","df0dc4d2fbb642239ba6b440d6dd8a8e","908a16942bb643e384437a7f2e5f3702","219944cfde7b4a4d8e5e232adbf60c25","5beb11a6fb5f405182960022f177c98c","736923a438a64886a4a18d264bd7f19e","457a37de29d34f24a27b43550cd90576","cb5d83ae94d2419eb510acd56c0f7a15","964e70cf7bdf47c4a4995081b046b742"]},"id":"Y0VjodiDK9y2","outputId":"3d0871a4-b841-4911-e4b9-d6b2b9674ba6","executionInfo":{"status":"ok","timestamp":1644107546591,"user_tz":-60,"elapsed":2195662,"user":{"displayName":"Dennis Räk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjmIDdhHVE1sZFkuKIXI5ugoxSEv6EEXo6UAq9Z9w=s64","userId":"06483127116991032650"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#@markdown #**Fire up the AI**\n","\n","#@markdown ---\n","from urllib.request import urlopen\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","if texts:\n","    print('Using texts:', texts)\n","if target_images:\n","    print('Using image prompts:', target_images)\n","if args.seed is None:\n","    seed = torch.seed()\n","else:\n","    seed = args.seed\n","torch.manual_seed(seed)\n","print('Using seed:', seed)\n","\n","model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n","perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n","# clock=deepcopy(perceptor.visual.positional_embedding.data)\n","# perceptor.visual.positional_embedding.data = clock/clock.max()\n","# perceptor.visual.positional_embedding.data=clamp_with_grad(clock,0,1)\n","\n","cut_size = perceptor.visual.input_resolution\n","\n","f = 2**(model.decoder.num_resolutions - 1)\n","make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n","\n","toksX, toksY = args.size[0] // f, args.size[1] // f\n","sideX, sideY = toksX * f, toksY * f\n","\n","if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","    e_dim = 256\n","    n_toks = model.quantize.n_embed\n","    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n","else:\n","    e_dim = model.quantize.e_dim\n","    n_toks = model.quantize.n_e\n","    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","# z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","# z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","\n","# normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","#                                            std=[0.229, 0.224, 0.225])\n","\n","if args.init_image:\n","    if 'http' in args.init_image:\n","      img = Image.open(urlopen(args.init_image))\n","    else:\n","      img = Image.open(args.init_image)\n","    pil_image = img.convert('RGB')\n","    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n","    pil_tensor = TF.to_tensor(pil_image)\n","    z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n","else:\n","    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n","    # z = one_hot @ model.quantize.embedding.weight\n","    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","        z = one_hot @ model.quantize.embed.weight\n","    else:\n","        z = one_hot @ model.quantize.embedding.weight\n","    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n","    z = torch.rand_like(z)*2\n","z_orig = z.clone()\n","z.requires_grad_(True)\n","opt = optim.Adam([z], lr=args.step_size)\n","\n","normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                  std=[0.26862954, 0.26130258, 0.27577711])\n","\n","\n","\n","pMs = []\n","\n","for prompt in args.prompts:\n","    txt, weight, stop = parse_prompt(prompt)\n","    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for prompt in args.image_prompts:\n","    path, weight, stop = parse_prompt(prompt)\n","    img = Image.open(path)\n","    pil_image = img.convert('RGB')\n","    img = resize_image(pil_image, (sideX, sideY))\n","    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n","    embed = perceptor.encode_image(normalize(batch)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n","    gen = torch.Generator().manual_seed(seed)\n","    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n","    pMs.append(Prompt(embed, weight).to(device))\n","\n","def synth(z):\n","    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n","    else:\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n","    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n","\n","@torch.no_grad()\n","def checkin(i, losses):\n","    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n","    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n","    out = synth(z)\n","    TF.to_pil_image(out[0].cpu()).save('progress.png')\n","    display.display(display.Image('progress.png'))\n","\n","def ascend_txt():\n","    global i\n","    out = synth(z)\n","    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n","    \n","    result = []\n","\n","    if args.init_weight:\n","        # result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n","        result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*args.init_weight) / 2)\n","    for prompt in pMs:\n","        result.append(prompt(iii))\n","    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n","    img = np.transpose(img, (1, 2, 0))\n","    imageio.imwrite('./steps/' + str(i) + '.png', np.array(img))\n","\n","    return result\n","\n","def train(i):\n","    opt.zero_grad()\n","    lossAll = ascend_txt()\n","    if i % args.display_freq == 0:\n","        checkin(i, lossAll)\n","       \n","    loss = sum(lossAll)\n","    loss.backward()\n","    opt.step()\n","    with torch.no_grad():\n","        z.copy_(z.maximum(z_min).minimum(z_max))\n","\n","i = 0\n","try:\n","    with tqdm() as pbar:\n","        while True:\n","            train(i)\n","            if i == max_iterations:\n","                break\n","            i += 1\n","            pbar.update()\n","except KeyboardInterrupt:\n","    pass"]}],"metadata":{"accelerator":"GPU","colab":{"name":"Analysis of Art - Reproduction.ipynb","provenance":[],"authorship_tag":"ABX9TyODdM/UeizQakgHjhsMOBHZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5fb0728f22b44f0e910cd3e414c79c99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c6ec54b2a8c542ebbe0b827484b12846","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e6d4bb1c6344799be3d62e8f9e7725d","IPY_MODEL_63a2430a64a04e1291866f8866d07c15","IPY_MODEL_f47b2945c09d4d6fa31bb869d66cf9b6"]}},"c6ec54b2a8c542ebbe0b827484b12846":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e6d4bb1c6344799be3d62e8f9e7725d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1c48006e2dee4033b808cb2216d7ca14","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e75663fa07144d6b8b0b784d4c42d6e"}},"63a2430a64a04e1291866f8866d07c15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fc891e7ddb9943a88a9803673999917f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553433881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce5bd45b0b6e4ed2b955e7eb879198c4"}},"f47b2945c09d4d6fa31bb869d66cf9b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ed1e9e38d3de4f7e8741ed6384f1b9a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [00:08&lt;00:00, 73.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e2eff22a0264fdf8189ccb4d3d27a0c"}},"1c48006e2dee4033b808cb2216d7ca14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5e75663fa07144d6b8b0b784d4c42d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc891e7ddb9943a88a9803673999917f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ce5bd45b0b6e4ed2b955e7eb879198c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ed1e9e38d3de4f7e8741ed6384f1b9a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4e2eff22a0264fdf8189ccb4d3d27a0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b46fc455e5a4750aff9334ae3029f38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_14c541b96cbd4026afd45258e3634f07","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9e669996c88d4668a7298dd89d61e1a2","IPY_MODEL_df0dc4d2fbb642239ba6b440d6dd8a8e","IPY_MODEL_908a16942bb643e384437a7f2e5f3702"]}},"14c541b96cbd4026afd45258e3634f07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9e669996c88d4668a7298dd89d61e1a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_219944cfde7b4a4d8e5e232adbf60c25","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5beb11a6fb5f405182960022f177c98c"}},"df0dc4d2fbb642239ba6b440d6dd8a8e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_736923a438a64886a4a18d264bd7f19e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_457a37de29d34f24a27b43550cd90576"}},"908a16942bb643e384437a7f2e5f3702":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cb5d83ae94d2419eb510acd56c0f7a15","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 757/? [36:05&lt;00:00,  2.86s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_964e70cf7bdf47c4a4995081b046b742"}},"219944cfde7b4a4d8e5e232adbf60c25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5beb11a6fb5f405182960022f177c98c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"736923a438a64886a4a18d264bd7f19e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"457a37de29d34f24a27b43550cd90576":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb5d83ae94d2419eb510acd56c0f7a15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"964e70cf7bdf47c4a4995081b046b742":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}